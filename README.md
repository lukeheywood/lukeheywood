# ONE â€” GitHub Front Door

### Coherence, Governance, and Systems That Hold Under Load

Hi, Iâ€™m Luke. I design and build AI-enabled systems that stay **coherent, inspectable, and governed** under real-world complexity.

This GitHub is the **front door** to the ONE system â€” a personal AI operating system built around memory, workflows, diagnostics, and explicit constraints.

This is not a product showcase.
Itâ€™s a **working system** with clear boundaries, runnable engines, and enforceable limits.

---

## ğŸ§­ The System at a Glance

ONE is intentionally split across a small number of repositories, each with a single, explicit role:

### ğŸ§± one-platform â€” *Canonical Truth*

The authoritative system map.

* System Atlas (what exists, what runs, whatâ€™s partial)
* Codex (design rationale and intent)
* Contracts index and governance surfaces
* Canonical case studies and evidence

If you want to understand **what ONE is and why itâ€™s shaped this way**, start here.

ğŸ‘‰ [https://github.com/lukeheywood/one-platform](https://github.com/lukeheywood/one-platform)

---

### âš™ï¸ one-runtime â€” *Runnable Public Mirror*

The system you can actually run.

* Memory OS, Autopilot, Meta OS, Orchestrator, Control Tower UI
* Governed workflows and inspection tooling
* Fixture-driven, fully reconstructable
* Zero private data, zero personal state

This repo proves the system works â€” safely and inspectably.

ğŸ‘‰ [https://github.com/lukeheywood/one-runtime](https://github.com/lukeheywood/one-runtime)

---

### ğŸ›¡ï¸ contract-stack â€” *Governance Authority*

The rules that bind the system.

* Formal contracts and admissibility rules
* Regression and coherence gates
* Enforcement utilities used by the runtime

If `one-runtime` shows **how** the system runs,
`contract-stack` defines **where it must stop**.

ğŸ‘‰ [https://github.com/lukeheywood/contract-stack](https://github.com/lukeheywood/contract-stack)

---

### ğŸ›ï¸ system â€” governance â€” framework 
System-level constitutional reference

A public, inspectable framework for governing complex systems where power, automation, and human responsibility intersect.

This framework defines the **structural conditions that must be true** before, during, and after a system operates â€” regardless of technology.

It is intentionally **not implementation guidance**, **not policy**, and **not an enforcement mechanism**.

---

## What this framework does

â€¢ Defines system-level governance boundaries (authority, ownership, accountability)
â€¢ Makes unacceptable system states explicit and legible
â€¢ Separates governance from implementation to prevent circular authority
â€¢ Provides a neutral diagnostic lens for assessing system legitimacy
â€¢ Remains valid across technologies, organisations, and time

---

## What this framework is not

â€¢ Not a product or platform
â€¢ Not a compliance checklist
â€¢ Not a moral or values manifesto
â€¢ Not specific to AI, ML, or any single technology
â€¢ Not dependent on any system, including ONE

---

## How to read it

This repository is a **constitutional layer**.

It exists independently of any system and can be used to inspect whether a system is structurally complete and governable.

AI systems, socio-technical platforms, and automated decision systems may be evaluated **against** this framework â€” but never define it.

The framework itself remains stable, public, and read-only.

---

ğŸ”— Canonical reference:
[https://github.com/lukeheywood/system-governance-framework](https://github.com/lukeheywood/system-governance-framework)

---

### ğŸ” system-diagnostics â€” *Drift & Failure Patterns*

Supporting patterns for understanding why systems degrade over time.

* Drift detection
* Competing intents
* Hidden coupling
* Quiet failure modes

This repo is not canonical, but it reflects how I reason about failure in complex platforms.

ğŸ‘‰ [https://github.com/lukeheywood/system-diagnostics](https://github.com/lukeheywood/system-diagnostics)

---

## ğŸ§  The Lens

Across all of this work, a few principles stay fixed:

* **Coherence under load**
  Systems must remain understandable even as complexity and pressure increase.

* **Governance by design**
  Constraints, invariants, and contracts prevent slow drift.

* **AI as pipelines, not prompts**
  LLMs are components inside workflows, not autonomous decision-makers.

* **Explicit limits and abstention**
  A system that knows when it must stop is safer than one that always answers.

* **Inspection over vibes**
  What exists, what runs, and whatâ€™s missing should be visible in black and white.

---

## ğŸ“š About Other Repositories

You may notice additional archived or experimental repositories on this account.

These are preserved for historical reference, pattern extraction, or prior exploration.
They are **not authoritative** and are intentionally de-emphasized.

The active system is represented by the repositories above.

---

## âœï¸ Authorship & Responsibility

ONE is an authored system.

I am its designer and take responsibility for its behavior, limits, and failure modes as it evolves.

Its assumptions are explicit.
Its constraints are intentional.
Its gaps are named, not obscured.

This work is built in public so it can be examined directly.

---

If youâ€™re interested in:

* AI systems that donâ€™t quietly drift,
* governance that actually constrains behavior,
* or making complex platforms legible again,

youâ€™re in the right place.
